{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Py_Launcher.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/akuzubasli/stanford-tensorflow-tutorials/blob/master/Py_Launcher.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "xc2x6RBfWXXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Git Commands"
      ]
    },
    {
      "metadata": {
        "id": "I_9Xs2MMPbK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8549ceae-9254-4fb4-92e4-a00f8fb0d14f"
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Counting objects: 4, done.\u001b[K\r\n",
            "remote: Compressing objects: 100% (1/1)   \u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\r\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/akuzubasli/stanford-tensorflow-tutorials\n",
            "   7073cff..3720999  master     -> origin/master\n",
            "Updating 7073cff..3720999\n",
            "Fast-forward\n",
            " examples/07_convnet_mnist_starter.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 2 insertions(+), 2 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYRnAXQXYA3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad47e4ba-0dd3-42d2-fd24-dd4a7cc5b61f"
      },
      "cell_type": "code",
      "source": [
        "cd ~/stanford-tensorflow-tutorials/examples/"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/stanford-tensorflow-tutorials/examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d__dW7wQk78r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "f32d3485-6604-498f-8a16-0f62a5907ac6"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02_lazy_loading.py            07_convnet_layers.py\r\n",
            "02_placeholder.py             07_convnet_mnist_export.py\r\n",
            "02_simple_tf.py               07_convnet_mnist.py\r\n",
            "02_variables.py               07_convnet_mnist_starter.py\r\n",
            "03_linreg_dataset.py          07_run_kernels.py\r\n",
            "03_linreg_placeholder.py      11_char_rnn.py\r\n",
            "03_linreg_starter.py          \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\r\n",
            "03_logreg_placeholder.py      \u001b[01;34mdata\u001b[0m/\r\n",
            "03_logreg.py                  \u001b[01;34mexported5\u001b[0m/\r\n",
            "03_logreg_starter.py          export.zip\r\n",
            "04_linreg_eager.py            \u001b[01;34mgraphs\u001b[0m/\r\n",
            "04_linreg_eager_starter.py    kernels.py\r\n",
            "04_word2vec_eager.py          \u001b[01;32mngrok\u001b[0m*\r\n",
            "04_word2vec_eager_starter.py  ngrok-stable-linux-amd64.zip\r\n",
            "04_word2vec.py                \u001b[01;34m__pycache__\u001b[0m/\r\n",
            "04_word2vec_visualize.py      utils.py\r\n",
            "05_randomization.py           word2vec_utils.py\r\n",
            "05_variable_sharing.py\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nz6OD_rEXBbI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rm -r checkpoints/ graphs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yj2eQ3x-k0lj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"export.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhETkdmxbskb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "897c6eeb-6124-4072-caa8-5decd90635ad"
      },
      "cell_type": "code",
      "source": [
        "!yes|git push git@github.com:akuzubasli/stanford-tensorflow-tutorials/examples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The authenticity of host 'github.com (192.30.253.112)' can't be established.\r\n",
            "RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.\n",
            "Are you sure you want to continue connecting (yes/no)? ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0drPg0yDan91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "878698d4-8f0f-4a8d-c3d3-e6fb127a6300"
      },
      "cell_type": "code",
      "source": [
        "!python3 07_convnet_mnist_export.py ./exported5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n",
            "  from ._conv import register_converters as _register_converters\n",
            "data/mnist/train-images-idx3-ubyte.gz already exists\n",
            "data/mnist/train-labels-idx1-ubyte.gz already exists\n",
            "data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
            "data/mnist/t10k-labels-idx1-ubyte.gz already exists\n",
            "WARNING:tensorflow:From 07_convnet_mnist_export.py:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Loss at step 3459: 0.001566733350045979\n",
            "Loss at step 3479: 0.0008734030998311937\n",
            "Loss at step 3499: 0.0002420684031676501\n",
            "Loss at step 3519: 0.06617140024900436\n",
            "Loss at step 3539: 0.019795089960098267\n",
            "Loss at step 3559: 0.0014419263461604714\n",
            "Loss at step 3579: 0.000921116559766233\n",
            "Loss at step 3599: 0.012389945797622204\n",
            "Loss at step 3619: 0.022531410679221153\n",
            "Loss at step 3639: 0.00012761953985318542\n",
            "Loss at step 3659: 0.0119557473808527\n",
            "Loss at step 3679: 0.014791623689234257\n",
            "Loss at step 3699: 0.0053679365664720535\n",
            "Loss at step 3719: 0.0005493795033544302\n",
            "Loss at step 3739: 0.0002555107057560235\n",
            "Loss at step 3759: 0.0026050074957311153\n",
            "Loss at step 3779: 0.01463364064693451\n",
            "Loss at step 3799: 0.006142818834632635\n",
            "Loss at step 3819: 0.0010576152708381414\n",
            "Loss at step 3839: 0.003766994457691908\n",
            "Loss at step 3859: 0.022222718223929405\n",
            "Average loss at epoch 0: 0.006588051653586242\n",
            "Took: 9.4082190990448 seconds\n",
            "Accuracy at epoch 0: 0.9924 \n",
            "Took: 0.6062920093536377 seconds\n",
            "Exporting trained model to b'./exported5/1'\n",
            "Done exporting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wF_-kR8BXOOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f68feab-3f46-4b27-b52f-b4c4d2004a41"
      },
      "cell_type": "code",
      "source": [
        "%%writefile 07_convnet_mnist_export.py\n",
        "\"\"\" Using convolutional net on MNIST dataset of handwritten digits\n",
        "MNIST dataset: http://yann.lecun.com/exdb/mnist/\n",
        "CS 20: \"TensorFlow for Deep Learning Research\"\n",
        "cs20.stanford.edu\n",
        "Chip Huyen (chiphuyen@cs.stanford.edu)\n",
        "Lecture 07\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import utils\n",
        "\n",
        "tf.app.flags.DEFINE_integer('training_iteration', 15,\n",
        "                            'number of training iterations.')\n",
        "tf.app.flags.DEFINE_integer('model_version', 1, 'version number of the model.')\n",
        "tf.app.flags.DEFINE_string('work_dir', '/tmp', 'Working directory.')\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "class ConvNet(object):\n",
        "    def __init__(self):\n",
        "        self.lr = 0.001\n",
        "        self.batch_size = 128\n",
        "        self.keep_prob = tf.constant(0.75)\n",
        "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
        "                                trainable=False, name='global_step')\n",
        "        self.n_classes = 10\n",
        "        self.skip_step = 20\n",
        "        self.n_test = 10000\n",
        "        self.training=False\n",
        "\n",
        "    def get_data(self):\n",
        "        with tf.name_scope('data'):\n",
        "            train_data, test_data = utils.get_mnist_dataset(self.batch_size)\n",
        "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
        "                                                   train_data.output_shapes)\n",
        "            img, self.label = iterator.get_next()\n",
        "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
        "            # reshape the image to make it work with tf.nn.conv2d\n",
        "\n",
        "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
        "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
        "\n",
        "    def inference(self):\n",
        "        conv1 = tf.layers.conv2d(inputs=self.img,\n",
        "                                  filters=32,\n",
        "                                  kernel_size=[5, 5],\n",
        "                                  padding='SAME',\n",
        "                                  activation=tf.nn.relu,\n",
        "                                  name='conv1')\n",
        "        pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
        "                                        pool_size=[2, 2], \n",
        "                                        strides=2,\n",
        "                                        name='pool1')\n",
        "\n",
        "        conv2 = tf.layers.conv2d(inputs=pool1,\n",
        "                                  filters=64,\n",
        "                                  kernel_size=[5, 5],\n",
        "                                  padding='SAME',\n",
        "                                  activation=tf.nn.relu,\n",
        "                                  name='conv2')\n",
        "        pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
        "                                        pool_size=[2, 2], \n",
        "                                        strides=2,\n",
        "                                        name='pool2')\n",
        "\n",
        "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
        "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
        "        fc = tf.layers.dense(pool2, 1024, activation=tf.nn.relu, name='fc')\n",
        "        dropout = tf.layers.dropout(fc, \n",
        "                                    self.keep_prob, \n",
        "                                    training=self.training, \n",
        "                                    name='dropout')\n",
        "        self.logits = tf.layers.dense(dropout, self.n_classes, name='logits')\n",
        "\n",
        "    def loss(self):\n",
        "        '''\n",
        "        define loss function\n",
        "        use softmax cross entropy with logits as the loss function\n",
        "        compute mean cross entropy, softmax is applied internally\n",
        "        '''\n",
        "        # \n",
        "        with tf.name_scope('loss'):\n",
        "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
        "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
        "    \n",
        "    def optimize(self):\n",
        "        '''\n",
        "        Define training op\n",
        "        using Adam Gradient Descent to minimize cost\n",
        "        '''\n",
        "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
        "                                                global_step=self.gstep)\n",
        "\n",
        "    def summary(self):\n",
        "        '''\n",
        "        Create summaries to write on TensorBoard\n",
        "        '''\n",
        "        with tf.name_scope('summaries'):\n",
        "            tf.summary.scalar('loss', self.loss)\n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            tf.summary.histogram('histogram loss', self.loss)\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "    \n",
        "    def eval(self):\n",
        "        '''\n",
        "        Count the number of right predictions in a batch\n",
        "        '''\n",
        "        with tf.name_scope('predict'):\n",
        "            preds = tf.nn.softmax(self.logits)\n",
        "            self.preds = preds\n",
        "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
        "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "    def build(self):\n",
        "        '''\n",
        "        Build the computation graph\n",
        "        '''\n",
        "        self.get_data()\n",
        "        self.inference()\n",
        "        self.loss()\n",
        "        self.optimize()\n",
        "        self.eval()\n",
        "        self.summary()\n",
        "\n",
        "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
        "        start_time = time.time()\n",
        "        sess.run(init) \n",
        "        self.training = True\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
        "                writer.add_summary(summaries, global_step=step)\n",
        "                if (step + 1) % self.skip_step == 0:\n",
        "                    print('Loss at step {0}: {1}'.format(step, l))\n",
        "                step += 1\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        saver.save(sess, 'checkpoints/convnet_layers/mnist-convnet', step)\n",
        "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
        "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
        "        return step\n",
        "\n",
        "    def eval_once(self, sess, init, writer, epoch, step):\n",
        "        start_time = time.time()\n",
        "        sess.run(init)\n",
        "        self.training = False\n",
        "        total_correct_preds = 0\n",
        "        try:\n",
        "            while True:\n",
        "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
        "                writer.add_summary(summaries, global_step=step)\n",
        "                total_correct_preds += accuracy_batch\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "\n",
        "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
        "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
        "\n",
        "    def train(self, n_epochs):\n",
        "        '''\n",
        "        The train function alternates between training one epoch and evaluating\n",
        "        '''\n",
        "        utils.safe_mkdir('checkpoints')\n",
        "        utils.safe_mkdir('checkpoints/convnet_layers')\n",
        "        writer = tf.summary.FileWriter('./graphs/convnet_layers', tf.get_default_graph())\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_layers/checkpoint'))\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            \n",
        "            step = self.gstep.eval()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
        "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
        "                \n",
        "                \n",
        "            # Export model\n",
        "            # WARNING(break-tutorial-inline-code): The following code snippet is\n",
        "            # in-lined in tutorials, please update tutorial documents accordingly\n",
        "            # whenever code changes.\n",
        "            export_path_base = sys.argv[-1]\n",
        "            export_path = os.path.join(\n",
        "                tf.compat.as_bytes(export_path_base),\n",
        "                tf.compat.as_bytes(str(FLAGS.model_version)))\n",
        "            print('Exporting trained model to', export_path)\n",
        "            builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
        "\n",
        "            # Build the signature_def_map.\n",
        "            classification_inputs = tf.saved_model.utils.build_tensor_info(\n",
        "                self.img)\n",
        "            classification_outputs_classes = tf.saved_model.utils.build_tensor_info(\n",
        "                self.preds)\n",
        "            classification_outputs_scores = tf.saved_model.utils.build_tensor_info(self.logits)\n",
        "\n",
        "            classification_signature = (\n",
        "                tf.saved_model.signature_def_utils.build_signature_def(\n",
        "                    inputs={\n",
        "                        tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n",
        "                            classification_inputs\n",
        "                    },\n",
        "                    outputs={\n",
        "                        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n",
        "                            classification_outputs_classes,\n",
        "                        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n",
        "                            classification_outputs_scores\n",
        "                    },\n",
        "                    method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n",
        "\n",
        "            tensor_info_x = tf.saved_model.utils.build_tensor_info(self.img)\n",
        "            tensor_info_y = tf.saved_model.utils.build_tensor_info(self.label)\n",
        "\n",
        "            prediction_signature = (\n",
        "                tf.saved_model.signature_def_utils.build_signature_def(\n",
        "                    inputs={'images': tensor_info_x},\n",
        "                    outputs={'scores': tensor_info_y},\n",
        "                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
        "\n",
        "            legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
        "            builder.add_meta_graph_and_variables(\n",
        "                sess, [tf.saved_model.tag_constants.SERVING],\n",
        "                signature_def_map={\n",
        "                    'predict_images':\n",
        "                        prediction_signature,\n",
        "                    tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
        "                        classification_signature,\n",
        "                },\n",
        "                legacy_init_op=legacy_init_op)\n",
        "\n",
        "            builder.save()\n",
        "\n",
        "            print('Done exporting!')    \n",
        "        writer.close()\n",
        "        \n",
        "    def export1(self):\n",
        "      if len(sys.argv) < 2 or sys.argv[-1].startswith('-'):\n",
        "        print('Usage: mnist_export.py [--training_iteration=x] '\n",
        "              '[--model_version=y] export_dir')\n",
        "        sys.exit(-1)\n",
        "      if FLAGS.training_iteration <= 0:\n",
        "        print('Please specify a positive value for training iteration.')\n",
        "        sys.exit(-1)\n",
        "      if FLAGS.model_version <= 0:\n",
        "        print('Please specify a positive value for version number.')\n",
        "        sys.exit(-1)\n",
        "\n",
        " \n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  model = ConvNet()\n",
        "  model.export1()\n",
        "  model.build()\n",
        "  model.train(n_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting 07_convnet_mnist_export.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UiG5YQMyWDK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tensorboard on Colab using ngrok\n",
        "\n",
        "https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab"
      ]
    },
    {
      "metadata": {
        "id": "MBsWB9x4YVIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Get TensorBoard running in the background."
      ]
    },
    {
      "metadata": {
        "id": "hU2e0GurYSEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCMIdOWUYasA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3.Launch ngrok background process...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vQ1NK7VkSdnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RahnlRjS0J_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa029e0c-0a8f-467f-cadb-3fec5d8f280c"
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://b5c37502.ngrok.io\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}